{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c870fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from utils import download_qwen3_small, Qwen3Tokenizer\n",
    "from qwen3 import Qwen3Model, QWEN_CONFIG_06_B, KVCache\n",
    "import torchinfo\n",
    "from typing import Optional, Generator, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb0a232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def set_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(device=\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(device=\"mps\")\n",
    "    else:\n",
    "        return torch.device(device=\"cpu\")\n",
    "\n",
    "\n",
    "device = set_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd3382",
   "metadata": {},
   "source": [
    "# 2.4 Preparing input texts for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd21d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_file_path = Path(\"qwen3\") / \"tokenizer-base.json\"\n",
    "tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22bf10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[840, 20772, 3460, 4128, 4119, 13]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain large language models.\"\n",
    "input_token_ids_list = tokenizer.encode(prompt)\n",
    "\n",
    "print(input_token_ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec0cc8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain large language models.\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(input_token_ids_list)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b246ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[840] --> Ex\n",
      "[20772] --> plain\n",
      "[3460] -->  large\n",
      "[4128] -->  language\n",
      "[4119] -->  models\n",
      "[13] --> .\n"
     ]
    }
   ],
   "source": [
    "for i in input_token_ids_list:\n",
    "    print(f\"{[i]} --> {tokenizer.decode([i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2f0ebd",
   "metadata": {},
   "source": [
    "Exercise 2.1: Encoding unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bea1a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68210] --> Cou\n",
      "[22249] --> cou\n",
      "[11] --> ,\n",
      "[9765] -->  tu\n",
      "[5208] -->  ve\n",
      "[2200] --> ux\n",
      "[45031] -->  voir\n",
      "[7491] -->  ma\n",
      "[22721] -->  bite\n",
      "[30] --> ?\n"
     ]
    }
   ],
   "source": [
    "french_token_ids_list = tokenizer.encode(prompt=\"Coucou, tu veux voir ma bite?\")\n",
    "\n",
    "for i in french_token_ids_list:\n",
    "    print(f\"{[i]} --> {tokenizer.decode([i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1049c315",
   "metadata": {},
   "source": [
    "# 2.5 Loading pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83ae3399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ qwen3/qwen3-0.6B-base.pth already up-to-date\n",
      "✓ qwen3/tokenizer-base.json already up-to-date\n"
     ]
    }
   ],
   "source": [
    "download_qwen3_small(kind=\"base\", tokenizer_only=False, out_dir=\"qwen3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "363a83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"qwen3\") / \"qwen3-0.6B-base.pth\"\n",
    "model = Qwen3Model(cfg=QWEN_CONFIG_06_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec36865",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, how are you today?\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "input_ids = input_ids.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdcf539a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "Qwen3Model (Qwen3Model)                       [1, 7]               [1, 7, 151936]       --                   True\n",
       "├─Embedding (tok_emb)                         [1, 7]               [1, 7, 1024]         155,582,464          True\n",
       "├─ModuleList (trf_blocks)                     --                   --                   --                   True\n",
       "│    └─TransformerBlock (0)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (1)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (2)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (3)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (4)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (5)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (6)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (7)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (8)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (9)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (10)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (11)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (12)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (13)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (14)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (15)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (16)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (17)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (18)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (19)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (20)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (21)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (22)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (23)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (24)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (25)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (26)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (27)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "├─RMSNorm (final_norm)                        [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "├─Linear (out_head)                           [1, 7, 1024]         [1, 7, 151936]       155,582,464          True\n",
       "=============================================================================================================================\n",
       "Total params: 751,632,384\n",
       "Trainable params: 751,632,384\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 751.57\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 17.96\n",
       "Params size (MB): 1503.40\n",
       "Estimated Total Size (MB): 1521.36\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "\n",
    "torchinfo.summary(\n",
    "    model=model,\n",
    "    input_data=input_ids,\n",
    "    verbose=0,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a795b",
   "metadata": {},
   "source": [
    "# 2.6 Understanding the sequential LLM text generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e80f2678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input tokens: 6\n",
      "Formatted Output tensor shape: torch.Size([6, 151936])\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain large language models.\"\n",
    "input_token_ids_list = tokenizer.encode(prompt)\n",
    "print(f\"Number of input tokens: {len(input_token_ids_list)}\")\n",
    " \n",
    "input_tensor = torch.tensor(input_token_ids_list)\n",
    "input_tensor_fmt = input_tensor.unsqueeze(0)\n",
    "input_tensor_fmt = input_tensor_fmt.to(device)\n",
    " \n",
    "output_tensor = model(input_tensor_fmt)\n",
    "output_tensor_fmt = output_tensor.squeeze(0)\n",
    "print(f\"Formatted Output tensor shape: {output_tensor_fmt.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0e708c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.3438,  2.0312,  7.9375,  ..., -2.5156, -2.5156, -2.5156],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "last_token = output_tensor_fmt[-1].detach()\n",
    "print(last_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2b0ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20286], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(last_token.argmax(dim=-1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac219df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([20286]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855bb9d",
   "metadata": {},
   "source": [
    "# 2.7 Coding a minimal text generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e42c1064",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_basic(model: Qwen3Model, token_ids: torch.Tensor, \n",
    "                        max_new_tokens: int, eos_token_id: Optional[int]=None) -> torch.Tensor:\n",
    "    input_length = token_ids.shape[1]\n",
    "    model.eval()\n",
    " \n",
    "    for _ in range(max_new_tokens):\n",
    "        out = model(token_ids)[:, -1]\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    " \n",
    "        if (eos_token_id is not None and torch.all(next_token == eos_token_id)):\n",
    "            break\n",
    " \n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "    \n",
    "    return token_ids[:, input_length:]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb166c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform tasks such as answering questions, writing text, and even creating music.<|endoftext|>Human language is a complex and dynamic system that has evolved over millions of years to enable effective communication and social interaction. Large language models are designed to mimic this complexity and adapt to new contexts and languages, making them powerful tools for a wide range of applications, from customer service to scientific research.<|endoftext|>Human language is a\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain large language models in a single sentence.\"\n",
    "input_token_ids_tensor = torch.tensor(tokenizer.encode(prompt), device=device).unsqueeze(0)\n",
    " \n",
    "max_new_tokens = 100\n",
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")\n",
    "\n",
    "output_text = tokenizer.decode(token_ids=output_token_ids_tensor.squeeze(0).tolist())\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d678bdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151643]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"<|endoftext|>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df6ff59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform tasks such as answering questions, writing text, and even creating music.\n"
     ]
    }
   ],
   "source": [
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "output_text = tokenizer.decode(token_ids=output_token_ids_tensor.squeeze(0).tolist())\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9b2e0",
   "metadata": {},
   "source": [
    "Exercise 2.2: Streaming token generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b900e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_modified(model: Qwen3Model, token_ids: torch.Tensor, \n",
    "                        max_new_tokens: int, eos_token_id: Optional[int]=None) -> Generator[Tuple[torch.Tensor, torch.Tensor]] :\n",
    "\n",
    "    model.eval()\n",
    " \n",
    "    for _ in range(max_new_tokens):\n",
    "        out = model(token_ids)[:, -1]\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    "        probas = torch.softmax(input=out, dim=-1)\n",
    "        max_proba = torch.max(input=probas)\n",
    "        yield next_token, max_proba\n",
    " \n",
    "        if (eos_token_id is not None and torch.all(next_token == eos_token_id)):\n",
    "            break\n",
    "\n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f0577f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large --> 20286 --> 61.25 %\n",
      " language --> 4128 --> 85.00 %\n",
      " models --> 4119 --> 99.50 %\n",
      " are --> 525 --> 77.50 %\n",
      " artificial --> 20443 --> 22.75 %\n",
      " intelligence --> 11229 --> 86.50 %\n",
      " systems --> 5942 --> 76.00 %\n",
      " that --> 429 --> 63.75 %\n",
      " can --> 646 --> 34.00 %\n",
      " understand --> 3535 --> 52.75 %\n",
      ", --> 11 --> 48.75 %\n",
      " generate --> 6923 --> 51.25 %\n",
      ", --> 11 --> 94.50 %\n",
      " and --> 323 --> 98.50 %\n",
      " process --> 1882 --> 21.62 %\n",
      " human --> 3738 --> 58.25 %\n",
      " language --> 4128 --> 95.00 %\n",
      ", --> 11 --> 34.75 %\n",
      " enabling --> 27362 --> 41.50 %\n",
      " them --> 1105 --> 83.00 %\n",
      " to --> 311 --> 100.00 %\n",
      " perform --> 2736 --> 34.00 %\n",
      " tasks --> 9079 --> 45.00 %\n",
      " such --> 1741 --> 59.00 %\n",
      " as --> 438 --> 100.00 %\n",
      " answering --> 35764 --> 27.50 %\n",
      " questions --> 4755 --> 99.00 %\n",
      ", --> 11 --> 97.50 %\n",
      " writing --> 4378 --> 38.00 %\n",
      " text --> 1467 --> 40.50 %\n",
      ", --> 11 --> 99.50 %\n",
      " and --> 323 --> 90.00 %\n",
      " even --> 1496 --> 43.75 %\n",
      " creating --> 6825 --> 36.00 %\n",
      " music --> 4627 --> 17.88 %\n",
      ". --> 13 --> 36.25 %\n",
      "<|endoftext|> --> 151643 --> 89.00 %\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text_modified(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "for token, proba in generated_text:\n",
    "    text = tokenizer.decode(token_ids=list(token))\n",
    "    id = token.item()\n",
    "    print(f\"{text} --> {id} --> {100 * proba:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1dc6bb",
   "metadata": {},
   "source": [
    "End of Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dede4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats(output_token_ids: torch.Tensor, tokenizer: Qwen3Tokenizer, start_time: float, end_time: float) -> None:\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Time: {total_time:.2f} sec\")\n",
    "    print(f\"{int(output_token_ids.numel() / total_time)} tokens/sec\")\n",
    " \n",
    "    max_mem_bytes = torch.mps.current_allocated_memory()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Current MPS memory allocated: {max_mem_gb:.2f} GB\")\n",
    " \n",
    "    output_text = tokenizer.decode(token_ids=output_token_ids.squeeze(0).tolist())\n",
    "    print(f\"\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f54873fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.90 sec\n",
      "12 tokens/sec\n",
      "Current MPS memory allocated: 1.46 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform tasks such as answering questions, writing text, and even creating music.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790c45c",
   "metadata": {},
   "source": [
    "# 2.8 Faster inference via KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce6fe6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_basic_cache(\n",
    "    model: Qwen3Model,\n",
    "    token_ids: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    eos_token_id: Optional[int]=None\n",
    ") -> torch.Tensor:\n",
    " \n",
    "    input_length = token_ids.shape[1]\n",
    "    model.eval()\n",
    "    cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "    out = model(token_ids, cache=cache)[:, -1]\n",
    " \n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    " \n",
    "        if (eos_token_id is not None and torch.all(next_token == eos_token_id)):\n",
    "            break\n",
    " \n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "        out = model(next_token, cache=cache)[:, -1]\n",
    " \n",
    "    return token_ids[:, input_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4e0d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.85 sec\n",
      "18 tokens/sec\n",
      "Current MPS memory allocated: 1.46 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing essays.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "output_token_ids_tensor = generate_text_basic_cache(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "end_time = time.time()\n",
    "generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a967e1",
   "metadata": {},
   "source": [
    "# 2.9 Faster inference via PyTorch model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b03ae22e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.compile is not supported on Python 3.14+",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m torch._dynamo.config.allow_unspec_int_on_nn_module = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model_compiled = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/llm/.venv/lib/python3.14/site-packages/torch/__init__.py:2590\u001b[39m, in \u001b[36mcompile\u001b[39m\u001b[34m(model, fullgraph, dynamic, backend, mode, options, disable)\u001b[39m\n\u001b[32m   2588\u001b[39m _C._log_api_usage_once(\u001b[33m\"\u001b[39m\u001b[33mtorch.compile\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2589\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys.version_info >= (\u001b[32m3\u001b[39m, \u001b[32m14\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2590\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtorch.compile is not supported on Python 3.14+\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sysconfig.get_config_var(\u001b[33m\"\u001b[39m\u001b[33mPy_GIL_DISABLED\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sys.version_info < (\n\u001b[32m   2592\u001b[39m     \u001b[32m3\u001b[39m,\n\u001b[32m   2593\u001b[39m     \u001b[32m13\u001b[39m,\n\u001b[32m   2594\u001b[39m     \u001b[32m3\u001b[39m,\n\u001b[32m   2595\u001b[39m ):\n\u001b[32m   2596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2597\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtorch.compile is not supported on Python < 3.13.3 built with GIL disabled. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2598\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use Python 3.13.3+.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2599\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: torch.compile is not supported on Python 3.14+"
     ]
    }
   ],
   "source": [
    "torch._dynamo.config.allow_unspec_int_on_nn_module = True\n",
    "\n",
    "model_compiled = torch.compile(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
