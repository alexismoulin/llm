{
 "cells": [
  {
   "cell_type": "code",
   "id": "53c870fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:32.485959Z",
     "start_time": "2025-11-22T20:32:26.306370Z"
    }
   },
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from utils import download_qwen3_small, Qwen3Tokenizer\n",
    "from qwen3 import Qwen3Model, QWEN_CONFIG_06_B, KVCache\n",
    "import torchinfo\n",
    "from typing import Optional, Generator, Tuple"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2fb0a232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:32.514991Z",
     "start_time": "2025-11-22T20:32:32.493717Z"
    }
   },
   "source": [
    "def set_device() -> torch.device:\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(device=\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(device=\"mps\")\n",
    "    else:\n",
    "        return torch.device(device=\"cpu\")\n",
    "\n",
    "\n",
    "device = set_device()\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "cabd3382",
   "metadata": {},
   "source": [
    "# 2.4 Preparing input texts for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd21d370",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:32.662706Z",
     "start_time": "2025-11-22T20:32:32.522007Z"
    }
   },
   "source": [
    "tokenizer_file_path = Path(\"qwen3\") / \"tokenizer-base.json\"\n",
    "tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_file_path)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c22bf10e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:32.669581Z",
     "start_time": "2025-11-22T20:32:32.667160Z"
    }
   },
   "source": [
    "prompt = \"Explain large language models.\"\n",
    "input_token_ids_list = tokenizer.encode(prompt)\n",
    "\n",
    "print(input_token_ids_list)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[840, 20772, 3460, 4128, 4119, 13]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "ec0cc8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:32.685145Z",
     "start_time": "2025-11-22T20:32:32.682976Z"
    }
   },
   "source": [
    "text = tokenizer.decode(token_ids=input_token_ids_list)\n",
    "print(text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain large language models.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "b246ccef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:32.696299Z",
     "start_time": "2025-11-22T20:32:32.693914Z"
    }
   },
   "source": [
    "for i in input_token_ids_list:\n",
    "    print(f\"{[i]} --> {tokenizer.decode([i])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[840] --> Ex\n",
      "[20772] --> plain\n",
      "[3460] -->  large\n",
      "[4128] -->  language\n",
      "[4119] -->  models\n",
      "[13] --> .\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "8c2f0ebd",
   "metadata": {},
   "source": [
    "Exercise 2.1: Encoding unknown words"
   ]
  },
  {
   "cell_type": "code",
   "id": "1bea1a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:32.711050Z",
     "start_time": "2025-11-22T20:32:32.708626Z"
    }
   },
   "source": [
    "french_token_ids_list = tokenizer.encode(prompt=\"Coucou, tu veux voir ma bite?\")\n",
    "\n",
    "for i in french_token_ids_list:\n",
    "    print(f\"{[i]} --> {tokenizer.decode([i])}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68210] --> Cou\n",
      "[22249] --> cou\n",
      "[11] --> ,\n",
      "[9765] -->  tu\n",
      "[5208] -->  ve\n",
      "[2200] --> ux\n",
      "[45031] -->  voir\n",
      "[7491] -->  ma\n",
      "[22721] -->  bite\n",
      "[30] --> ?\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1049c315",
   "metadata": {},
   "source": [
    "# 2.5 Loading pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "id": "83ae3399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:33.442679Z",
     "start_time": "2025-11-22T20:32:32.722956Z"
    }
   },
   "source": [
    "download_qwen3_small(kind=\"base\", tokenizer_only=False, out_dir=\"qwen3\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ qwen3/qwen3-0.6B-base.pth already up-to-date\n",
      "✓ qwen3/tokenizer-base.json already up-to-date\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "363a83ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:36.976530Z",
     "start_time": "2025-11-22T20:32:33.448053Z"
    }
   },
   "source": [
    "model_path = Path(\"qwen3\") / \"qwen3-0.6B-base.pth\"\n",
    "model = Qwen3Model(cfg=QWEN_CONFIG_06_B)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "dec36865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:36.985282Z",
     "start_time": "2025-11-22T20:32:36.980084Z"
    }
   },
   "source": [
    "text = \"Hello, how are you today?\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "input_ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "input_ids = input_ids.to(device)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "fdcf539a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:37.902865Z",
     "start_time": "2025-11-22T20:32:36.989168Z"
    }
   },
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "\n",
    "torchinfo.summary(\n",
    "    model=model,\n",
    "    input_data=input_ids,\n",
    "    verbose=0,\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=============================================================================================================================\n",
       "Layer (type (var_name))                       Input Shape          Output Shape         Param #              Trainable\n",
       "=============================================================================================================================\n",
       "Qwen3Model (Qwen3Model)                       [1, 7]               [1, 7, 151936]       --                   True\n",
       "├─Embedding (tok_emb)                         [1, 7]               [1, 7, 1024]         155,582,464          True\n",
       "├─ModuleList (trf_blocks)                     --                   --                   --                   True\n",
       "│    └─TransformerBlock (0)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (1)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (2)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (3)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (4)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (5)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (6)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (7)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (8)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (9)                   [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (10)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (11)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (12)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (13)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (14)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (15)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (16)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (17)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (18)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (19)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (20)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (21)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (22)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (23)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (24)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (25)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (26)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "│    └─TransformerBlock (27)                  [1, 7, 1024]         [1, 7, 1024]         --                   True\n",
       "│    │    └─RMSNorm (norm1)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─GroupedQueryAttention (att)       [1, 7, 1024]         [1, 7, 1024]         6,291,712            True\n",
       "│    │    └─RMSNorm (norm2)                   [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "│    │    └─FeedForward (ff)                  [1, 7, 1024]         [1, 7, 1024]         9,437,184            True\n",
       "├─RMSNorm (final_norm)                        [1, 7, 1024]         [1, 7, 1024]         1,024                True\n",
       "├─Linear (out_head)                           [1, 7, 1024]         [1, 7, 151936]       155,582,464          True\n",
       "=============================================================================================================================\n",
       "Total params: 751,632,384\n",
       "Trainable params: 751,632,384\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 751.57\n",
       "=============================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 17.96\n",
       "Params size (MB): 1503.40\n",
       "Estimated Total Size (MB): 1521.36\n",
       "============================================================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "e56a795b",
   "metadata": {},
   "source": [
    "# 2.6 Understanding the sequential LLM text generation process"
   ]
  },
  {
   "cell_type": "code",
   "id": "e80f2678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:38.555753Z",
     "start_time": "2025-11-22T20:32:37.911335Z"
    }
   },
   "source": [
    "prompt = \"Explain large language models.\"\n",
    "input_token_ids_list = tokenizer.encode(prompt)\n",
    "print(f\"Number of input tokens: {len(input_token_ids_list)}\")\n",
    " \n",
    "input_tensor = torch.tensor(input_token_ids_list)\n",
    "input_tensor_fmt = input_tensor.unsqueeze(0)\n",
    "input_tensor_fmt = input_tensor_fmt.to(device)\n",
    " \n",
    "output_tensor = model(input_tensor_fmt)\n",
    "output_tensor_fmt = output_tensor.squeeze(0)\n",
    "print(f\"Formatted Output tensor shape: {output_tensor_fmt.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input tokens: 6\n",
      "Formatted Output tensor shape: torch.Size([6, 151936])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "fc0e708c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:40.463072Z",
     "start_time": "2025-11-22T20:32:38.561402Z"
    }
   },
   "source": [
    "last_token = output_tensor_fmt[-1].detach()\n",
    "print(last_token)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 7.3438,  2.0312,  7.9375,  ..., -2.5156, -2.5156, -2.5156],\n",
      "       device='mps:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "f2b0ac82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:40.485914Z",
     "start_time": "2025-11-22T20:32:40.477804Z"
    }
   },
   "source": [
    "print(last_token.argmax(dim=-1, keepdim=True))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20286], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "3ac219df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:40.498375Z",
     "start_time": "2025-11-22T20:32:40.496184Z"
    }
   },
   "source": [
    "print(tokenizer.decode([20286]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "1855bb9d",
   "metadata": {},
   "source": [
    "# 2.7 Coding a minimal text generation function"
   ]
  },
  {
   "cell_type": "code",
   "id": "e42c1064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:40.514758Z",
     "start_time": "2025-11-22T20:32:40.512495Z"
    }
   },
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_basic(model: Qwen3Model, token_ids: torch.Tensor, \n",
    "                        max_new_tokens: int, eos_token_id: Optional[int]=None) -> torch.Tensor:\n",
    "    input_length = token_ids.shape[1]\n",
    "    model.eval()\n",
    " \n",
    "    for _ in range(max_new_tokens):\n",
    "        out = model(token_ids)[:, -1]\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    " \n",
    "        if eos_token_id is not None and torch.all(next_token == eos_token_id):\n",
    "            break\n",
    " \n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "    \n",
    "    return token_ids[:, input_length:]\n",
    " "
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "eb166c21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:54.479163Z",
     "start_time": "2025-11-22T20:32:40.529827Z"
    }
   },
   "source": [
    "prompt = \"Explain large language models in a single sentence.\"\n",
    "input_token_ids_tensor = torch.tensor(tokenizer.encode(prompt), device=device).unsqueeze(0)\n",
    " \n",
    "max_new_tokens = 100\n",
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    ")\n",
    "\n",
    "output_text = tokenizer.decode(token_ids=output_token_ids_tensor.squeeze(0).tolist())\n",
    "print(output_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform tasks such as answering questions, writing text, and even creating music.<|endoftext|>Human language is a complex and dynamic system that has evolved over millions of years to enable effective communication and social interaction. Large language models are designed to mimic this complexity and adapt to new contexts and languages, making them powerful tools for a wide range of applications, from customer service to scientific research.<|endoftext|>Human language is a\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "d678bdac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:54.494243Z",
     "start_time": "2025-11-22T20:32:54.491816Z"
    }
   },
   "source": [
    "print(tokenizer.encode(\"<|endoftext|>\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151643]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "df6ff59b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:57.257316Z",
     "start_time": "2025-11-22T20:32:54.516719Z"
    }
   },
   "source": [
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "output_text = tokenizer.decode(token_ids=output_token_ids_tensor.squeeze(0).tolist())\n",
    "print(output_text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform tasks such as answering questions, writing text, and even creating music.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "b6d9b2e0",
   "metadata": {},
   "source": [
    "Exercise 2.2: Streaming token generation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b900e29a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:57.266095Z",
     "start_time": "2025-11-22T20:32:57.262281Z"
    }
   },
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_modified_cached(\n",
    "    model: Qwen3Model, token_ids: torch.Tensor, max_new_tokens: int, eos_token_id: Optional[int] = None\n",
    ") -> Generator[Tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    \n",
    "    model.eval()\n",
    "    cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "\n",
    "    # First forward on the full prompt\n",
    "    logits: torch.Tensor = model(token_ids, cache=cache)[:, -1, :]\n",
    "\n",
    "    finished = torch.zeros(token_ids.size(0), dtype=torch.bool, device=token_ids.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token: torch.Tensor = logits.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "        # Prob of the chosen token per sample\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        chosen_prob = probs.gather(-1, next_token)\n",
    "        yield next_token.squeeze(-1), chosen_prob.squeeze(-1)\n",
    "\n",
    "        if eos_token_id is not None:\n",
    "            finished |= (next_token.squeeze(-1) == eos_token_id)\n",
    "            if finished.all().item():\n",
    "                break\n",
    "\n",
    "        # Feed only the new token; cache carries the past\n",
    "        logits = model(next_token, cache=cache)[:, -1, :]"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "4f0577f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:57.282570Z",
     "start_time": "2025-11-22T20:32:57.280957Z"
    }
   },
   "source": [
    "generated_text = generate_text_modified_cached(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "62782222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:59.392715Z",
     "start_time": "2025-11-22T20:32:57.288823Z"
    }
   },
   "source": [
    "for token, proba in generated_text:\n",
    "    tok_id: int = int(token.item())\n",
    "    p: float = proba.item()\n",
    "    piece: str = tokenizer.decode([tok_id])\n",
    "    print(f\"{piece} --> {tok_id} --> {p*100:.2f} %\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Large --> 20286 --> 61.33 %\n",
      " language --> 4128 --> 85.16 %\n",
      " models --> 4119 --> 99.61 %\n",
      " are --> 525 --> 76.56 %\n",
      " artificial --> 20443 --> 21.00 %\n",
      " intelligence --> 11229 --> 84.77 %\n",
      " systems --> 5942 --> 78.12 %\n",
      " that --> 429 --> 62.50 %\n",
      " can --> 646 --> 35.35 %\n",
      " understand --> 3535 --> 53.12 %\n",
      ", --> 11 --> 48.83 %\n",
      " generate --> 6923 --> 50.39 %\n",
      ", --> 11 --> 93.36 %\n",
      " and --> 323 --> 98.44 %\n",
      " process --> 1882 --> 22.66 %\n",
      " human --> 3738 --> 59.77 %\n",
      " language --> 4128 --> 94.92 %\n",
      ", --> 11 --> 35.74 %\n",
      " enabling --> 27362 --> 41.02 %\n",
      " them --> 1105 --> 83.98 %\n",
      " to --> 311 --> 100.00 %\n",
      " perform --> 2736 --> 34.57 %\n",
      " a --> 264 --> 43.75 %\n",
      " wide --> 6884 --> 95.31 %\n",
      " range --> 2088 --> 97.66 %\n",
      " of --> 315 --> 100.00 %\n",
      " tasks --> 9079 --> 90.62 %\n",
      ", --> 11 --> 23.14 %\n",
      " from --> 504 --> 85.16 %\n",
      " answering --> 35764 --> 55.86 %\n",
      " questions --> 4755 --> 98.05 %\n",
      " to --> 311 --> 84.38 %\n",
      " writing --> 4378 --> 50.39 %\n",
      " essays --> 22844 --> 28.12 %\n",
      ". --> 13 --> 30.66 %\n",
      "<|endoftext|> --> 151643 --> 92.58 %\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "bc1dc6bb",
   "metadata": {},
   "source": [
    "End of Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "id": "6dede4ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:32:59.398415Z",
     "start_time": "2025-11-22T20:32:59.396192Z"
    }
   },
   "source": [
    "def generate_stats(output_token_ids: torch.Tensor, tokenizer: Qwen3Tokenizer, start_time: float, end_time: float) -> None:\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"Time: {total_time:.2f} sec\")\n",
    "    print(f\"{int(output_token_ids.numel() / total_time)} tokens/sec\")\n",
    " \n",
    "    max_mem_bytes = torch.mps.current_allocated_memory()\n",
    "    max_mem_gb = max_mem_bytes / (1024 ** 3)\n",
    "    print(f\"Current MPS memory allocated: {max_mem_gb:.2f} GB\")\n",
    " \n",
    "    output_text = tokenizer.decode(token_ids=output_token_ids.squeeze(0).tolist())\n",
    "    print(f\"\\n{output_text}\")"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "f54873fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:33:02.128274Z",
     "start_time": "2025-11-22T20:32:59.404049Z"
    }
   },
   "source": [
    "start_time = time.time()\n",
    "output_token_ids_tensor = generate_text_basic(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.72 sec\n",
      "13 tokens/sec\n",
      "Current MPS memory allocated: 1.46 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform tasks such as answering questions, writing text, and even creating music.\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "e790c45c",
   "metadata": {},
   "source": [
    "# 2.8 Faster inference via KV caching"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce6fe6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:33:02.136640Z",
     "start_time": "2025-11-22T20:33:02.133756Z"
    }
   },
   "source": [
    "@torch.inference_mode()\n",
    "def generate_text_basic_cache(\n",
    "    model: Qwen3Model,\n",
    "    token_ids: torch.Tensor,\n",
    "    max_new_tokens: int,\n",
    "    eos_token_id: Optional[int]=None\n",
    ") -> torch.Tensor:\n",
    " \n",
    "    input_length = token_ids.shape[1]\n",
    "    model.eval()\n",
    "    cache = KVCache(n_layers=model.cfg[\"n_layers\"])\n",
    "    model.reset_kv_cache()\n",
    "    out = model(token_ids, cache=cache)[:, -1]\n",
    " \n",
    "    for _ in range(max_new_tokens):\n",
    "        next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
    " \n",
    "        if eos_token_id is not None and torch.all(next_token == eos_token_id):\n",
    "            break\n",
    " \n",
    "        token_ids = torch.cat([token_ids, next_token], dim=1)\n",
    "        out = model(next_token, cache=cache)[:, -1]\n",
    " \n",
    "    return token_ids[:, input_length:]"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "a4e0d0bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:33:03.746408Z",
     "start_time": "2025-11-22T20:33:02.149323Z"
    }
   },
   "source": [
    "start_time = time.time()\n",
    "output_token_ids_tensor = generate_text_basic_cache(\n",
    "    model=model,\n",
    "    token_ids=input_token_ids_tensor,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "end_time = time.time()\n",
    "generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.59 sec\n",
      "21 tokens/sec\n",
      "Current MPS memory allocated: 1.46 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing essays.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "96a967e1",
   "metadata": {},
   "source": [
    "# 2.9 Faster inference via PyTorch model compilation"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:33:07.169172Z",
     "start_time": "2025-11-22T20:33:03.752747Z"
    }
   },
   "cell_type": "code",
   "source": "model_compiled = torch.compile(model)",
   "id": "14b0ff4627a0fa7b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:34:04.957972Z",
     "start_time": "2025-11-22T20:33:25.879811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    output_token_ids_tensor = generate_text_basic(\n",
    "        model=model_compiled,\n",
    "        token_ids=input_token_ids_tensor,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"Warm-up run\")\n",
    "    else:\n",
    "        print(f\"Timed run {i}:\")\n",
    "    generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)\n",
    "\n",
    "    print(f\"\\n{30*'-'}\\n\")"
   ],
   "id": "95507c860c12583b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1122 21:33:32.072000 4402 .venv/lib/python3.11/site-packages/torch/_inductor/utils.py:1613] [0/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up run\n",
      "Time: 34.66 sec\n",
      "1 tokens/sec\n",
      "Current MPS memory allocated: 1.46 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing essays, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Timed run 1:\n",
      "Time: 2.19 sec\n",
      "18 tokens/sec\n",
      "Current MPS memory allocated: 1.46 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing essays, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Timed run 2:\n",
      "Time: 2.22 sec\n",
      "18 tokens/sec\n",
      "Current MPS memory allocated: 1.46 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing essays, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T20:42:11.733823Z",
     "start_time": "2025-11-22T20:42:05.349354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(3):\n",
    "    start_time = time.time()\n",
    "    output_token_ids_tensor = generate_text_basic_cache(\n",
    "        model=model_compiled,\n",
    "        token_ids=input_token_ids_tensor,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    if i == 0:\n",
    "        print(\"Warm-up run\")\n",
    "        generate_stats(\n",
    "        output_token_ids_tensor, tokenizer, start_time, end_time\n",
    "    )\n",
    "    else:\n",
    "        print(f\"Timed run {i}:\")\n",
    "        generate_stats(output_token_ids_tensor, tokenizer, start_time, end_time)\n",
    "\n",
    "    print(f\"\\n{30*'-'}\\n\")\n"
   ],
   "id": "8db35fdb67eac38e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm-up run\n",
      "Time: 2.63 sec\n",
      "15 tokens/sec\n",
      "Current MPS memory allocated: 1.47 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Timed run 1:\n",
      "Time: 1.86 sec\n",
      "22 tokens/sec\n",
      "Current MPS memory allocated: 1.47 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Timed run 2:\n",
      "Time: 1.89 sec\n",
      "21 tokens/sec\n",
      "Current MPS memory allocated: 1.47 GB\n",
      "\n",
      " Large language models are artificial intelligence systems that can understand, generate, and process human language, enabling them to perform a wide range of tasks, from answering questions to writing articles, and even creating creative content.\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
